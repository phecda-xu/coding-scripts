# 模型量化

量化在数字信号处理领域，是指将信号的连续取值（或者大量可能的离散取值）近似为有限多个（或较少的）离散值的过程。量化主要应用于从连续信号到数字信号的转换中。连续信号经过采样成为离散信号，离散信号经过量化即成为数字信号.

这里我们用来描述把模型的高精度的浮点(Floating-point)数权值转化为低精度的定点(Fixed-point)数权值的过程，例如把`float32` 精度的浮点数据转化为`int8`精度的定点数。量化之后，模型体积缩减为原来的`1/4`，计算速度也因为精度下降而变快。量化是主要模型压缩技术之一，主要用于将模型嵌入芯片的过程中，目的是缩减模型体积和提升计算速度。本文除非明确说明，一般默认为`int8`量化。



# 目录

- 量化的基本概念和原理
- 模型量化的主要方法策略
- 主流平台的量化实现



## 1、量化的基本概念和原理

### 1.1 浮点与定点

 		定点和浮点都是数值的表示（representation），它们区别在于，将整数（integer）部分和小数（fractional）部分分开的**点**，点在哪里。定点保留特定位数整数和小数，而浮点保留特定位数的*有效数字*（significand）和*指数*（exponent） 。比如：

|        | fixed point  | floating point       |
| ------ | ------------ | -------------------- |
| format |              |                      |
| 十进制 | 12345.678901 | 1.2345678901 x 10**4 |
| 二进制 | 10111.01011  | 1.011101011 x 2**4   |

​		定点数中，小数点左侧为整数，右侧为小数位；

​		浮点数中左侧为有效数字（significand）右侧为指数（exponent） 。

这里有几点需要说明：

- 1、定点数是在计算机中表示数字的一种方式，它既可以表示整数，也可以表示小数

- 2、在固定 bit 下，约定小数点的位置，然后把整数部分和小数部分分别转换为二进制，就是定点数的结果

- 3、受限于小数点的位置，用定点数表示小数时，数值的范围和小数精度是有限的

- 4、在现代计算机中，定点数通常用来表示整数，对于高精度的小数，通常用浮点数表示
- 5、模型量化中是将浮点小数转换为定点整数



###  1.2 Q格式

​		Q格式是二进制的定点数表示格式，表示形式为`Qm.n`和`UQm.n`其中的`.`即为小数点，m表示整数位的长度，n表示小数位的长度；`UQm.n`表示无符号数，值全为正整数，`Qm.n`为有符号数，值可以为正也可以为负，有符号的数据又根据符号位的处理方法不同分两种形式，虽然都是用`Qm.n`表示，但是可以通过计算`m+n`的值来进行区分，当`m+n`的值等于数字位元数时，表示其符号位元放在整数位元 m中，若比数字位元数少1，表示其其符号位元独立计算。 当n=0时，Q就是整数；当m=0时，Q就是小数，此时`m.`可以省略，即可以写成`Qn`

​		以`int8`类型的数据为例,常见的有符号数据和无符号数据类型如下:

| 格式  | 表示的浮点数值域 | 公式                                  | 二进制     | 量化表示范围 |
| ----- | ---------------- | ------------------------------------- | ---------- | ------------ |
| Q5.2  | [-16, 16.75]     | [-2 ** (m-1), 2\*\*(m-1) - 2\*\*(-n)] | 0111 11.11 | [-128, 127]  |
| UQ5.3 | [0, 31.75]       | [0, 2\*\*m - 2\*\*(-n)]               | 1111 1.111 | [0, 255]     |

### 1.3 scale 、zero_point以及dec_bits

​		`scale` 、`zero_point`以及`dec_bits`三个量化参数的计算与四个统计值有关，分别是r_max，r_min，q_max，q_min，即待量化数组的最大值和最小值，量化后定点数取值范围的最大值和最小值。

​		**`scale`** 是缩放系数，一般计算公式如下：
$$
S = {{r_\max - r_\min} \over {q_\max - q_\min}}
$$

$$
其中 r_\max 和 r_\min 是数组r的最大值和最小值， q_\max 和 q_\min 是量化后定点数取值范围最大值和最小值.
$$

​		**`zero_point`**，顾名思义就是零点的位置：
$$
Z = {round(q_\max - {r_\max \over S})} 
$$
​		定点整数的 zero_point 就代表浮点实数的 0，二者之间的换算不存在精度损失。 这么做的目的是为了在 padding 时保证浮点数值的 0 和定点整数的 zero_point 完全等价，保证定点和浮点之间的表征能够一致.

​		**`dec_bits`**是小数点的位置，计算公式如下：
$$
dec\_bits = {7 - ceil(log_2(max({|r_\max|, |r_\min|})))}
$$
​		有符号的`int8`的数据，其中`1-bit` 表示符号位。

### 1.4 量化与反量化

​		**`量化`**，`float32` 转 `int8`，用`scale`和`zero_point`计算公式如下：
$$
q = round({r \over S} + Z)
$$
用`dec_bits`计算公式如下：
$$
q = round(r * 2^{dec\_bits})
$$
​		**`反量化`**，把量化成`int8`的数组再反量化成`float32`的数组，用`scale`和`zero_point`计算公式如下：
$$
r = S * (q - Z)
$$
用`dec_bits`计算公式如下：
$$
r = {q \over 2^{dec\_bits}}
$$

### 1.5 两套量化计算体系

​		目前笔者见到的有两种量化计算体系，一个基于`scale`，`zero_point`的量化体系，一个是基于`dec_bits`移位的量化体系，某些条件下，两种量化体系等价。

- 例如：

```
浮点数据: r = [0.002, 0.458, 6.589, -1.256, -9.001]
量化目标：有符号int8, q_max=127, q_min=-128
max_val = 6.589, min_val = -9.001;

1、基于scale、zero_point进行直接量化：
scale = (max_val - min_val) / (q_max - q_min) = 0.06113725
zero_point = q_min - round(min_val/scale) = 19
q_1 = round(r/scale + zero_point) = [19, 26, 127, -10, -128]

2、基于dec_bits进行移位量化
int_bits = int(np.ceil(np.log2(max(abs(max_val), abs(min_val)))) = 4
dec_bits = 7-4=3
q_2 = round(r*2**dec_bits) = [0, 4, 53, -14, -72]

3、等价条件: zero_point=0
int_bits = int(np.ceil(np.log2(max(abs(max_val), abs(min_val)))) = 4
dec_bits = 7-4=3
new_max_val = 2**int_bits - 2 **(-dec_bits) = 2**4 - 2**(-3) = 15.879
new_min_val = - 2**int_bits = -2**4 = -16

new_scale = (new_max_val - new_min_val) / (q_max - q_min) = 0.125
zero_point = q_min - round(new_min_val/new_scale) = 0

q_3 = round(r/scale + zero_point) = [0, 4, 53, -14, -72] 
q_3 = q_2
此时可以看到，两种方式的量化结果一致；
```

​		基于`dec_bits`移位的量化方法默认隐含的条件是zero_point=0.所以当要量化的数组有正有负时，选择对称量化到[-128,127]，zero_point正好为0；当要量化的数组非负时，量化到[0,255]，zero_point也为0.

​		这两种情况下，满足等价条件，将数组的实际最大、最小值换为可表示范围的最大、最小值，两套量化计算的结果是一致的。



## 2、模型量化主要内容及方法策略

### 2.1 权值量化与激活量化

​		模型量化分为两部分，一个是`权值量化`，一个是网络输入与输出的量化，也叫`激活量化`。

- 权值量化

​        权值量化，当模型训练之后，权值一般固定下来不再变化，量化需要的统计参数`r_min`, `r_max`可以直接得到，所以一般这部分的量化可以按照量化公式直接进行计算。

- 为什么有激活量化？

​		**激活量化**不是**激活函数量化**，这里所说的激活量化是指矩阵运算后的输出，通常不包括激活函数（部分激活函数比较简单可以直接使用时可以包括在内），神经网络本质上是输入与权值两个矩阵之间的运算，输出即为运算结果。模型量化，将权值量化为`int8`，同样的输入数据也需要量化到相同精度下，矩阵之间在`int8`精度下完成计算后的输出也同样需要保持在该精度下, 因此对于输出，也要有确定的量化参数。具体可以参考如下公式[神经网络量化入门--基本原理](https://zhuanlan.zhihu.com/p/149659607)理解：

![1632378221611](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1632378221611.png)

- 激活量化的一般方法

​		模型的输出不是固定的，激活部分的统计参数`r_min`, `r_max`必须要对输入和输出的数据进行统计后才能得到，所以激活量化一般需要一定量的真实样本数据输入模型，得到对于的输出，然后从输出中统计出对应的`r_min`, `r_max`。



### 2.2 对称量化和非对称量化

​		所谓对称量化就是将权值量化到以0为对称中心的的值域范围内，比如量化成有符号数，量化范围取值[-128, 127]，且`zero_point = 0`，即为对称量化；但是也不是说`zero_point = 0`时就一定是对称量化，比如将非负的数组量化到[0,255]，此时满足`zero_point = 0`，但是显然它不是对称量化。

​		非对称量化的直观体现就是`zero_point != 0`,比如将有正有负的数组量化成无符号数，量化范围取值[0, 255]，此时`zero_point != 0`，即为非对称量化。

​		**有符号数和无符号数相乘，结果还是有符号数**；

​		**有符号数与无符号数相加，需要统一零点位置**；



### 2.2 训练后量化（Post Train Static Quantization，PTSQ）

​		这是最常见的模型量化方法，其量化过程即是`2.1`中两部分的具体实现。`PTSQ`首先进行**`权值量化`**，提取每一层的权值矩阵和偏置矩阵，由于模型是训练好的，权值和偏置都是固定的，数据范围可以确定，所以可以直接按照量化公式进行量化计算，得到量化后的权值和偏置。对神经网络的输入和输出的量化叫**`激活量化`**，由于输入和输出是未知的，所以这部分的量化需要少量的真实数据输入网络，并统计输入输出数组的数值范围，之后再计算对应的量化参数。



### 2.3 量化感知训练（Quantization Aware Training, QAT）

​		量化感知训练，顾名思义就是带训练的量化。量化将数组由高精度数据转为低精度数据，直接量化势必会造成模型效果的损失，在模型结构比较复杂的情况下，这种损失会非常大，所以就需要一种新的量化方法来减小损失。量化感知训练就是一种在量化的同时会根据量化引入的误差来调整模型权值的方法，通过这种训练量化可以有效的减小量化造成的效果损失，甚至能够带来效果提升。

## 3 量化工具

### 3.1 tf1

- 训练后量化

在`tf1`版本中提供了激活量化的`API`，即`tf.fake_quant_with_min_max_vars `.

根据[ML-KWS-for-MCU](https://github.com/ARM-software/ML-KWS-for-MCU/blob/master/quant_models.py)中的使用样例，如果模型包含`bn`层，则首先将`bn`与`conv`进行合并，然后采用`dec_bits`移位计算的方式对权值进行量化。将量化的权值再反量化回去，替换掉原来的权值（量化-反量化 之间存在误差）。然后使用`tf.fake_quant_with_min_max_vars `接口统计每层网络输出（激活）的值域范围，即量化可以表示的浮点数值范围[min, max]，源代码对应的是act_max参数，形式如下。

```
x = tf.fake_quant_with_min_max_vars(x, min=-act_max, \
               						max=act_max-(act_max/128.0), num_bits=8)
```

量化后的数值范围为`[-128, 127]`。act_max取值一般为2的指数（保证移位计算的操作和scale、zero_point计算结果一致），最大值可取`2**7`。每层对应一个act_max参数， act_max值确定后，量化的取值范围`[min, max]`也就确定了`[-act_max, act_max-(act_max/128.0)]`，在实际量化时，x的所有元素值会被限制在这个范围内（clip操作），并按照这个范围计算scale和zero_point，然后进行量化转化为`int8`型。

```
这个过程，可以换个角度讲就是，统计出激活部分的最大值和最小值，然后找到距离这个最大值、最小值最近的由2的指数计算的取值范围[-2**n, 2**n - (2**n/128.0)]，用这个范围计算量化参数scale和zero_point，然后用量化参数进行激活量化。(此时scale和zero_point量化计算和dec_bits移位量化计算是等价的，所以权值量化用dec_bits，激活量化用scale和zero_point)
```

- 量化感知训练（待完成）



### 3.2 tf2

待完成

### 3.3 pytorch[详细过程](https://zhuanlan.zhihu.com/p/299108528)

- 训练后量化

```
后端设置：torch.backends.quantized.engine = "qnnpack" / "fbgemm"

1，fuse_model
torch.quantization.fuse_modules(model, [['conv1', 'bn1', 'relu1']], inplace=True)

2, 设置qconfig
q-config: torch.quantization.get_default_qconfig('qnnpack')

3, prepare
torch.quantization.prepare(moderl, inplace=True)

4, 喂数据，不需要反向传播

5， finish quant
torch.quantization.convert(model.eval(), inplace=True)

模型部分：
self.quant_in = QuantStub()
self.dequant = DeQuantStub()
def forward（）：
    x = self.quant_in(x)
    ...
    x = self.dequant(x)
    return x
```

- 量化感知训练

```
过程与前面类似，部分改动
2, 设置qconfig
q-config: torch.quantization.get_default_qat_qconfig('qnnpack')

4， 喂数据，加反向传播
```





# 参考链接

1、[【什么是定点数】](https://zhuanlan.zhihu.com/p/338588296)

2、[【Q格式】](https://zh.wikipedia.org/wiki/Q%E6%A0%BC%E5%BC%8F)

3、[【pytorch的量化- Gemfield】](https://zhuanlan.zhihu.com/p/299108528)

4、[【神经网络量化入门--基本原理】](https://zhuanlan.zhihu.com/p/149659607)

5、[【神经网络量化入门--后训练量化】](https://zhuanlan.zhihu.com/p/156835141)

6、[【神经网络量化入门--量化感知训练】](https://zhuanlan.zhihu.com/p/158776813)

7、[【神经网络量化入门--Folding BN ReLU】](https://zhuanlan.zhihu.com/p/176982058)

8、[【神经网络量化入门--激活函数】](https://zhuanlan.zhihu.com/p/353440096)

9、[ tf.quantization.fake_quant_with_min_max_vars ](https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/quantization/fake_quant_with_min_max_vars?hl=enps://www.cnblogs.com/liaohuiqiang/p/7673681.html)
