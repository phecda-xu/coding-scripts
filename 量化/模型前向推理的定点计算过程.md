# 模型前向推力的定点化计算过程

模型量化分为权值量化和激活量化两部分,其中权值量化可以直接进行,激活量化则需要先在一定量的数据上进行统计.
本文为解释说明文档, 更多代码请参考 [lanxun_kws](http://192.168.1.10:7878/phecda-xu/lanxun_kws/-/blob/main/quantize.py#L67) ; 
此外若无特殊说明, 本文默认使用8位的定点数.

## 权值量化

以 tf 1.14.0 版本的模型为例, 提取每一层网络的权值然后进行定点化. 这一步主要计算整数位和小数位,以确定Q值.

```python
import numpy as np
import tensorflow as tf

...
for v in tf.trainable_variables():
    var_name = str(v.name)
    var_values = sess.run(v)  # 提取权值
    min_value = var_values.min() # 获取权值的最小值
    max_value = var_values.max() # 获取权值的最大值
    int_bits = int(np.ceil(np.log2(max(abs(min_value), abs(max_value))))) # 计算权值的最大整数位
    dec_bits = 7 - int_bits # 计算权值的小数位
    int_var_values = np.round(var_values * 2 ** dec_bits)

    # int_bits 的含义为 权值的取值范围是[- 2**int_bits, 2**int_bits - 2**int_bits/128]
    # 则, 适合权值的8位有符号定点数格式为 Q(int_bits, dec_bits) 其中 int_bits + dec_bits = 7;
    
    # 反量化,将"量化->反量化"之后的浮点权值重新赋值给模型,可以通过模型的表现来观察量化带来的误差
    float_var_values = int_var_values / (2 ** dec_bits)
    var_values = self.sess.run(tf.assign(v, float_var_values))

```

```
# 量化示例
# 浮点的偏执向量
var_values = [-0.25826364755630493, 0.6883572936058044, 0.8945991396903992, 0.7164932489395142, 1.3451591730117798, -0.42299747467041016, 0.11381220072507858, 0.05717455595731735, -0.7386460304260254, 0.7703797817230225, 0.5973451733589172, -0.625542163848877, 0.43592116236686707, -0.44198739528656006, -1.6033519506454468, 0.945207417011261, -0.14624227583408356, -0.33422982692718506, -0.1577053964138031, 0.8498015999794006, -0.04891720041632652, 0.4231269061565399, 0.5052178502082825, -0.6135373711585999, 0.013642233796417713, -0.16735133528709412, 0.6442906260490417, -0.2990519106388092, -1.3263283967971802, 0.6916907429695129, 0.2637832760810852, 0.49835988879203796, 0.12749871611595154, 0.3327294886112213, 0.4628201127052307, -0.03626351058483124, 0.6932054162025452, 0.667036235332489, 0.25013941526412964, -0.4056147336959839, 1.0198098421096802, 0.09326556324958801, 0.03928830102086067, 0.38358908891677856, -0.35011380910873413, -0.20912863314151764, 1.6707241535186768, -0.3117242753505707, 0.16182459890842438, 0.4877845048904419, 0.6321765780448914, 0.07444045692682266, 0.02494966983795166, -0.36230936646461487, 0.38685286045074463, -0.3203383684158325, -0.07053801417350769, 0.03180362284183502, 0.3626014292240143, 0.1907843053340912, 0.03814658895134926, -0.20613034069538116, 0.7572904229164124, -0.2638199031352997, -0.06000973656773567, 0.12071556597948074, -0.00590886315330863, 0.5353044271469116, 0.6171818971633911, -0.1631408929824829, 0.16440384089946747, -0.46566179394721985, 0.21591104567050934, -0.5146607160568237, 0.5357246398925781, -1.4957523345947266, 0.15953759849071503, 0.4537014961242676, -0.1904749572277069, 1.674491047859192, -0.16795848309993744, 0.2929626703262329, 0.2860981225967407, -0.493912935256958, -0.34932225942611694, -0.03489752486348152, -0.20735172927379608, -0.3077278733253479, 0.0347854346036911, 0.7094317078590393, 0.0890602320432663, 0.41826802492141724, -0.15558868646621704, 0.43935734033584595, -0.48966383934020996, -0.10104265064001083]
# 统计结果
max_value = 1.674491
min_value = -1.603352
int_bits = 1
dec_bits = 6

# 可得偏执向量的取值范围是 [-2, 1.984375]
# 量化的数据格式: Q1.6(Q6)
int_var_values = [-17, 44, 57, 46, 86, -27, 7, 4, -47, 49, 38, -40, 28, -28, -103, 60, -9, -21, -10, 54, -3, 27, 32, -39, 1, -11, 41, -19, -85, 44, 17, 32, 8, 21, 30, -2, 44, 43, 16, -26, 65, 6, 3, 25, -22, -13, 107, -20, 10, 31, 40, 5, 2, -23, 25, -21, -5, 2, 23, 12, 2, -13, 48, -17, -4, 8, 0, 34, 39, -10, 11, -30, 14, -33, 34, -96, 10, 29, -12, 107, -11, 19, 18, -32, -22, -2, -13, -20, 2, 45, 6, 27, -10, 28, -31, -6]

# 反量化
float_var_values = [-0.265625, 0.6875, 0.890625, 0.71875, 1.34375, -0.421875, 0.109375, 0.0625, -0.734375, 0.765625, 0.59375, -0.625, 0.4375, -0.4375, -1.609375, 0.9375, -0.140625, -0.328125, -0.15625, 0.84375, -0.046875, 0.421875, 0.5, -0.609375, 0.015625, -0.171875, 0.640625, -0.296875, -1.328125, 0.6875, 0.265625, 0.5, 0.125, 0.328125, 0.46875, -0.03125, 0.6875, 0.671875, 0.25, -0.40625, 1.015625, 0.09375, 0.046875, 0.390625, -0.34375, -0.203125, 1.671875, -0.3125, 0.15625, 0.484375, 0.625, 0.078125, 0.03125, -0.359375, 0.390625, -0.328125, -0.078125, 0.03125, 0.359375, 0.1875, 0.03125, -0.203125, 0.75, -0.265625, -0.0625, 0.125, -0.0, 0.53125, 0.609375, -0.15625, 0.171875, -0.46875, 0.21875, -0.515625, 0.53125, -1.5, 0.15625, 0.453125, -0.1875, 1.671875, -0.171875, 0.296875, 0.28125, -0.5, -0.34375, -0.03125, -0.203125, -0.3125, 0.03125, 0.703125, 0.09375, 0.421875, -0.15625, 0.4375, -0.484375, -0.09375]
```

```
# 整个模型的量化结果
cfsmn/dense1/kernel:0
shape: (264, 96)
int_bits: 0; dec_bits:7
cfsmn/dense1/bias:0
shape: (96,)
int_bits: 1; dec_bits:6
cfsmn/dense2/kernel:0
shape: (96, 64)
int_bits: 2; dec_bits:5
cfsmn/dense2/bias:0
shape: (64,)
int_bits: 0; dec_bits:7
cfsmn/dense3/kernel:0
shape: (64, 64)
int_bits: 2; dec_bits:5
cfsmn/dense3/bias:0
shape: (64,)
int_bits: 0; dec_bits:7
Variable:0
shape: (7, 64)
int_bits: 1; dec_bits:6
Variable_1:0
shape: (64, 66)
int_bits: 0; dec_bits:7
Variable_2:0
shape: (66,)
int_bits: 0; dec_bits:7
```


## 激活量化

- 模型代码

激活量化,需要在搭建模型代码中增加对输入和输出的处理, fake_quant_with_min_max_vars 根据最大和最小值进行的伪量化处理.

之所以叫伪量化,是应为模型推理实际还是用的浮点数据. 

统计最大值和最小值不是相互比较,而是人为按照 `[- 2**n, 2**n - 2**n/128]` 的设定设置最大最小值,然后根据模型在测试数据上的表现来决定是否使用该范围值;


```python
act_max = [0, 0, 0, 0, 0, 0] # act_max的每个元素值为 2**n , n为正整数, n<=7 , 如 2**7 = 128

x = tf.reshape(x, [1, 11 * feature_dim])
# 对输入数据进行量化参数统计
if act_max[0] > 0:
    x = tf.fake_quant_with_min_max_vars(x, min=-act_max[0], max=act_max[0]-(act_max[0]/128.0), num_bits=8)
with tf.variable_scope('cfsmn'):
    x = tf.layers.dense(x, units=96, name="dense1")
    # 对第1层的输出数据进行量化参数统计
    if act_max[1] > 0:
        x = tf.fake_quant_with_min_max_vars(x, min=-act_max[1], max=act_max[1] - (act_max[1] / 128.0), num_bits=8)
    x = tf.nn.relu(x)
    x = tf.layers.dense(x, units=64, name="dense2")
    # 对第2层的输出数据进行量化参数统计
    if act_max[2] > 0:
        x = tf.fake_quant_with_min_max_vars(x, min=-act_max[2], max=act_max[2] - (act_max[2] / 128.0), num_bits=8)
    x = tf.nn.relu(x)
    x = tf.layers.dense(x, units=64, name="dense3")
    # 对第3层的输出数据进行量化参数统计
    if act_max[3] > 0:
        x = tf.fake_quant_with_min_max_vars(x, min=-act_max[3], max=act_max[3] - (act_max[3] / 128.0), num_bits=8)
    x = tf.nn.relu(x)
x = tf.reshape(x, [1, 1, 64])
memory = tf.concat([x, state], axis=1)
Wa = tf.Variable(tf.truncated_normal([6 + 1, 64], stddev=0.1))
Wo = tf.Variable(tf.truncated_normal([64, 66], stddev=0.1))
bo = tf.Variable(tf.constant(0., shape=[66]))
Wa = tf.reshape(Wa, [1, 6 + 1, 64])
project = tf.multiply(memory, Wa)
project = tf.math.reduce_sum(project, axis=1)
# 对第4层的输出数据进行量化参数统计
if act_max[4] > 0:
    project = tf.fake_quant_with_min_max_vars(project, min=-act_max[4], max=act_max[4] - (act_max[4] / 128.0),
                                              num_bits=8)
before_state = memory[:, 0:6, :]
logits = tf.matmul(project, Wo) + bo
# 对第5层的输出数据进行量化参数统计
if act_max[5] > 0:
    logits = tf.fake_quant_with_min_max_vars(logits, min=-act_max[5], max=act_max[5] - (act_max[5] / 128.0),
                                             num_bits=8)
```

```
act_max 的元素从前往后每次只变动一个值,当前位置的值使得模型效果最好,那该位的值就固定下来不再变,继续变下一个位置的值.

act_max = [4, 16, 16, 16, 32, 32]

输入数据的格式: Q2.5
第1层输出的数据格式: Q4.3
第2层输出的数据格式: Q4.3
第3层输出的数据格式: Q4.3
第4层输出的数据格式: Q5.2
第5层输出的数据格式: Q5.2
```

## 前向推理

- 输入

```python
import numpy as np

# 从音频中提取的浮点特征
layer_input_float = np.array([-1.9929015701716641, -2.613190110521633, -2.535145446878486, -2.4286014118255035, -2.4798002169594566, -2.3520911159607185, -2.475176344904645, -2.235218725315987, -2.263958278145656, -2.3944889372464306, -2.427647279136558, -2.222927940019107, -2.1760334588184285, -2.1034803747741253, -2.1118491174198692, -2.1317679930850177, -2.047031844225774, -2.0217123813405116, -1.9287524317251827, -1.87239253318788, -1.8901265449791849, -1.7417159777180085, -1.7878134363237161, -1.7765780568312435, -1.9649872605033132, -2.7268772388058578, -2.6691958598928873, -2.514602578242702, -2.5236732375343465, -2.391158621219538, -2.5271309934043793, -2.240727782726559, -2.330476926826031, -2.410858575250428, -2.372039202961815, -2.22505466752665, -2.200145463235964, -2.139875589900951, -2.0905210023716094, -2.1295725353145687, -2.057405669405178, -1.9986409125308582, -1.9131048494780416, -1.9058872647789646, -1.9150200317131354, -1.7808380388634646, -1.8164334655877201, -1.7517838695605563, -2.043771885042903, -2.5467737641702746, -2.615568863646888, -2.6074883212145443, -2.5596834043997716, -2.3736748419739024, -2.469957848405635, -2.2123724374742744, -2.316067272226023, -2.520805949972943, -2.4019599718648514, -2.243230937208919, -2.21605352540065, -2.1583774031998937, -2.0839174253236106, -2.1338450916054006, -2.124025132406011, -2.0324121513589994, -1.987501195057217, -2.003429125167667, -1.9062924546756548, -1.8815539719082701, -1.7797964993460194, -1.748130207451612, -2.0878626715118775, -2.371667383632259, -2.615208862465889, -2.786481201433646, -2.570020366259993, -2.4035043601515698, -2.478685777529468, -2.395402065293222, -2.321632510832266, -2.4305282432513953, -2.372037147461693, -2.2936370159448947, -2.2206349272770005, -2.2371178143873793, -2.0873679311337994, -2.1936586262642668, -2.030740788186147, -1.9765072095530256, -1.9116748397903314, -1.9640436991597832, -1.8973108665572225, -1.7896963031062647, -1.7337874147070214, -1.720668351639917, -2.2083498841260276, -2.578229563061457, -2.619995661432479, -2.475246175260467, -2.5664232581675783, -2.479286825946426, -2.572101442741273, -2.453027084245408, -2.2774626708822217, -2.389167775627365, -2.4173892020711008, -2.3469360042566074, -2.248507550455778, -2.272386881715609, -2.1086792408709703, -2.1517817961560954, -2.049623815067317, -1.9583606820065438, -1.9601135349676286, -1.9542308822656238, -1.8933405822067095, -1.746756422420084, -1.714147083322568, -1.7179063884273362, -2.03600570037722, -2.2855311158577654, -2.433035794858117, -2.4030490505140762, -2.6017922283677217, -2.4980780866712053, -2.5638735382496356, -2.3125129903616575, -2.2838126357162407, -2.357281817622535, -2.439561764376831, -2.3336317044136816, -2.199796298875599, -2.2355126176654676, -2.228683969705529, -2.067381717827746, -2.0497669573181585, -1.9881362302593428, -1.9415148827480424, -1.9199149493785295, -1.8937364471482796, -1.786629462554556, -1.7172338578553075, -1.7224848960767531, -2.033707894515188, -2.5361478674150626, -2.603420637716687, -2.5352213219822795, -2.604972287488894, -2.488030465504697, -2.487071866423191, -2.3479533956815204, -2.230480865962378, -2.346279733741377, -2.437253582473421, -2.2480738472889814, -2.2589456538242074, -2.1907184575143135, -2.246603698828941, -2.053736446161368, -2.1027551772186155, -1.9816866997136868, -2.0136065858115617, -1.971884803064677, -1.8755402663953535, -1.7353409976763905, -1.6999126945717735, -1.7216975279443858, -2.0832255007311162, -2.3417940968212685, -2.6875057227477637, -2.558624198817631, -2.5819764131921388, -2.5842275128014665, -2.6279024057020894, -2.399389606728609, -2.3363798890230285, -2.4886632476654187, -2.325323467696319, -2.143362944740226, -2.1725685066594767, -2.104702633656835, -2.143915365639704, -2.077532755673202, -2.0429472543744835, -2.008966796837151, -2.108280866642554, -1.9485704064996259, -1.8536463298638737, -1.759256544646738, -1.6978083052417485, -1.7302451737377678, -1.9413040281990133, -2.193580285436857, -2.4423885496573243, -2.420550156150682, -2.45882181463196, -2.553164302317078, -2.65410995713797, -2.4273275556540237, -2.279518752950276, -2.3858099311544425, -2.3585951533994547, -2.1380236094978398, -2.1992563300979104, -2.089210197818482, -2.1873003665393047, -2.1253785867331447, -2.0886469701878188, -2.031100506484334, -1.9949351631926595, -1.8899451505882354, -1.8559541906190467, -1.7521381286777022, -1.7203634720434735, -1.7533469169640337, -2.164262989672964, -2.400230750165152, -2.376580847706575, -2.4008356133142743, -2.569838990620302, -2.6333063980791884, -2.5872957851768126, -2.4474870766319383, -2.287871748056236, -2.3286572471288527, -2.2821730198264625, -2.1432980237312456, -2.177003940166371, -2.1370429629476337, -2.235184932896819, -2.1717550033856923, -2.016755208146919, -2.06160739200653, -1.9975431386162934, -1.8876939857373418, -1.831689003257312, -1.7544147809220294, -1.76304213188371, -1.6917307937939676, -2.14643693735191, -2.3608722064146876, -2.3381358440776276, -2.3022963202523123, -2.467725137720422, -2.5052892300286342, -2.39453410003918, -2.443671898047907, -2.1939268441697384, -2.331160585452535, -2.451005453075584, -2.2120910148056896, -2.2035568281982596, -2.2057653825569563, -2.261226009723874, -2.1957474001329187, -2.0377839316118647, -2.15559779849039, -1.987687098530656, -1.9005146345669348, -1.8902995747295739, -1.790186538091915, -1.7605256534489993, -1.7178577513944286])
# 量化: 已知输入数据为 Q2.5
layer_input_q = (layer_input_float * 2 **5).astype(np.int32)
# 从音频中提取的定点特征
layer_input = np.array([-65, -84, -78, -76, -78, -74, -77, -72, -73, -78, -78, -72, -71, -68, -69, -70, -67, -65, -62, -61, -61, -57, -58, -58, -65, -80, -88, -80, -79, -76, -80, -72, -75, -77, -76, -72, -72, -69, -67, -70, -67, -64, -62, -62, -62, -58, -59, -57, -66, -79, -81, -80, -82, -78, -80, -71, -75, -81, -77, -72, -71, -70, -67, -70, -69, -66, -64, -65, -62, -61, -58, -57, -66, -75, -84, -90, -83, -78, -80, -78, -75, -78, -76, -74, -71, -72, -67, -71, -66, -64, -62, -64, -61, -58, -56, -56, -73, -81, -82, -79, -83, -81, -84, -79, -73, -79, -78, -75, -72, -73, -68, -70, -66, -63, -63, -63, -61, -57, -56, -56, -66, -74, -78, -76, -84, -80, -82, -75, -73, -77, -80, -75, -71, -73, -72, -67, -66, -64, -63, -62, -62, -58, -56, -56, -64, -81, -81, -78, -83, -81, -81, -77, -72, -77, -80, -72, -73, -71, -72, -66, -68, -64, -66, -64, -61, -56, -55, -56, -66, -73, -87, -81, -82, -83, -83, -79, -77, -81, -75, -69, -70, -68, -69, -67, -66, -65, -68, -63, -60, -57, -55, -56, -61, -69, -80, -76, -79, -81, -84, -77, -74, -77, -76, -69, -71, -67, -70, -68, -68, -66, -65, -61, -60, -57, -56, -57, -68, -73, -76, -78, -85, -85, -85, -79, -75, -75, -74, -69, -70, -69, -71, -70, -65, -67, -65, -61, -59, -57, -57, -55, -68, -76, -74, -76, -81, -81, -78, -79, -71, -75, -80, -71, -71, -71, -73, -71, -66, -70, -64, -62, -61, -58, -57, -56]).astype(np.int32)

before_state_float = np.zeros((6, 64), dtype=np.float32)

before_state = np.zeros((6, 64), dtype=np.int32)
```


```
layer_input_q = np.array([-63, -83, -81, -77, -79, -75, -79, -71, -72, -76, -77, -71, -69, -67, -67, -68, -65, -64, -61, -59, -60, -55, -57, -56, -62, -87, -85, -80, -80, -76, -80, -71, -74, -77, -75, -71, -70, -68, -66, -68, -65, -63, -61, -60, -61, -56, -58, -56, -65, -81, -83, -83, -81, -75, -79, -70, -74, -80, -76, -71, -70, -69, -66, -68, -67, -65, -63, -64, -61, -60, -56, -55, -66, -75, -83, -89, -82, -76, -79, -76, -74, -77, -75, -73, -71, -71, -66, -70, -64, -63, -61, -62, -60, -57, -55, -55, -70, -82, -83, -79, -82, -79, -82, -78, -72, -76, -77, -75, -71, -72, -67, -68, -65, -62, -62, -62, -60, -55, -54, -54, -65, -73, -77, -76, -83, -79, -82, -74, -73, -75, -78, -74, -70, -71, -71, -66, -65, -63, -62, -61, -60, -57, -54, -55, -65, -81, -83, -81, -83, -79, -79, -75, -71, -75, -77, -71, -72, -70, -71, -65, -67, -63, -64, -63, -60, -55, -54, -55, -66, -74, -86, -81, -82, -82, -84, -76, -74, -79, -74, -68, -69, -67, -68, -66, -65, -64, -67, -62, -59, -56, -54, -55, -62, -70, -78, -77, -78, -81, -84, -77, -72, -76, -75, -68, -70, -66, -69, -68, -66, -64, -63, -60, -59, -56, -55, -56, -69, -76, -76, -76, -82, -84, -82, -78, -73, -74, -73, -68, -69, -68, -71, -69, -64, -65, -63, -60, -58, -56, -56, -54, -68, -75, -74, -73, -78, -80, -76, -78, -70, -74, -78, -70, -70, -70, -72, -70, -65, -68, -63, -60, -60, -57, -56, -54])

before_state = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])

before_state_float = np.array([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                               [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                               [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                               [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                               [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                               [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])
```

- 第一层计算

```python
# 浮点计算
weight_1_float = np.load("save/cfsmn_dense1_kernel_0_float.npy")
bias_1_float = np.load("save/cfsmn_dense1_bias_0_float.npy")
layer_out_1_float = np.clip(np.matmul(weight_1_float, layer_input_float) + bias_1_float, a_min=0.0, a_max=2.0**32)
# 量化: 已知第1层输出的数据格式为 Q4.3
layer_out_1_q = (layer_out_1_float * 2**3).astype(np.int32)
# weight_1: Q7  bias_1: Q6 ;  (Q7 * Q5 + Q6<<6)>>9
weight_1 = np.load("save/cfsmn_dense1_kernel_0.npy").astype(np.int32)
bias_1 = np.load("save/cfsmn_dense1_bias_0.npy").astype(np.int32)
layer_out_1 = np.clip((np.matmul(weight_1, layer_input) + bias_1<<6)>>9, a_min=0, a_max=2**32)
```

```
layer_out_1_q = [0, 0, 35, 9, 36, 23, 0, 0, 0, 5, 0, 14, 7, 0, 0, 1, 1, 0, 0, 2, 0, 8, 9, 56, 0, 6, 8, 0, 4, 16, 0, 0, 13, 41, 5, 0, 0, 0, 28, 2, 0, 0, 0, 0, 0, 0, 10, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 23, 3, 6, 7, 0, 4, 0, 0, 9, 0, 6, 0, 3, 4, 8, 13, 5, 0, 27, 10, 0, 13, 16, 0, 1, 3, 0, 1, 0, 0, 0, 1, 13, 2, 0, 0, 12, 2, 0]

layer_out_1   = [0, 0, 34, 10, 36, 23, 0, 1, 1, 5, 0, 13, 9, 0, 0, 1, 1, 0, 0, 3, 0, 8, 9, 56, 0, 5, 8, 0, 4, 17, 0, 0, 12, 41, 5, 0, 0, 0, 26, 2, 0, 0, 0, 0, 0, 0, 9, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 22, 3, 7, 7, 0, 4, 0, 0, 9, 0, 5, 0, 2, 4, 8, 13, 4, 0, 28, 9, 0, 13, 16, 0, 2, 3, 0, 0, 0, 0, 0, 1, 13, 3, 0, 0, 12, 1, 0]
```


- 第二层计算

```python
# 浮点计算
weight_2_float = np.load("save/cfsmn_dense2_kernel_0_float.npy")
bias_2_float = np.load("save/cfsmn_dense2_bias_0_float.npy")
layer_out_2_float = np.clip(np.matmul(weight_2_float, layer_out_1_float) + bias_2_float,a_min=0.0, a_max=2.0**32)
# 量化: 已知第2层输出的数据格式为 Q4.3
layer_out_2_q = (layer_out_2_float * 2**3).astype(np.int32)
# weight_2: Q5  bias_2: Q7 ;  (Q5 * Q3 + Q7<<1)>>5
weight_2 = np.load("save/cfsmn_dense2_kernel_0.npy").astype(np.int32)
bias_2 = np.load("save/cfsmn_dense2_bias_0.npy").astype(np.int32)
layer_out_2 = np.clip((np.matmul(weight_2, layer_out_1) + bias_2<<1)>>5, a_min=0, a_max=2**32)
```

```
layer_out_2_q = [0, 19, 0, 0, 0, 0, 0, 17, 0, 1, 0, 16, 0, 8, 13, 36, 0, 12, 17, 5, 31, 0, 0, 0, 1, 0, 0, 7, 0, 2, 0, 0, 17, 0, 0, 0, 0, 0, 0, 18, 13, 23, 0, 36, 0, 19, 6, 4, 39, 26, 13, 1, 0, 0, 10, 14, 0, 0, 0, 0, 5, 0, 0, 0]

layer_out_2   = [0, 21, 0, 0, 0, 0, 0, 17, 0, 2, 0, 16, 0, 8, 14, 35, 0, 11, 15, 6, 30, 0, 0, 0, 0, 0, 0, 8, 0, 2, 1, 0, 16, 0, 1, 0, 0, 0, 0, 18, 12, 22, 0, 36, 0, 19, 6, 4, 37, 25, 13, 0, 0, 0,  9, 13, 0, 0, 0, 0, 6, 0, 0, 0]
```

- 第三层计算

```python
# 浮点计算
weight_3_float = np.load("save/cfsmn_dense3_kernel_0_float.npy")
bias_3_float = np.load("save/cfsmn_dense3_bias_0_float.npy")
layer_out_3_float = np.clip(np.matmul(weight_3_float, layer_out_2_float) + bias_3_float, a_min=0.0, a_max=2.0**32)
# 量化: 已知第3层输出的数据格式为 Q4.3
layer_out_3_q = (layer_out_3_float * 2**3).astype(np.int32)
# weight_3: Q5  bias_2: Q7 ;  (Q5 * Q3 + Q7<<1)>>5
weight_3 = np.load("save/cfsmn_dense3_kernel_0.npy").astype(np.int32)
bias_3 = np.load("save/cfsmn_dense3_bias_0.npy").astype(np.int32)
layer_out_3 = np.clip((np.matmul(weight_3, layer_out_2) + bias_3<<1)>>5, a_min=0, a_max=2**32)
```

```
layer_out_3_q = [0, 23, 0, 25, 28, 10, 34, 23, 4, 30, 26, 0, 23, 0, 21, 18, 52, 17, 10, 24, 4, 0, 0, 8, 0, 31, 0, 20, 13, 33, 17, 46, 11, 1, 33, 12, 6, 10, 19, 17, 35, 45, 15, 18, 12, 25, 8, 29, 0, 0, 20, 3, 28, 26, 15, 8, 13, 33, 0, 15, 19, 13, 63, 20]

layer_out_3   = [0, 22, 0, 24, 26,  9, 33, 23, 4, 29, 25, 0, 21, 0, 20, 17, 48, 16, 10, 23, 3, 0, 0, 9, 0, 30, 0, 19, 12, 31, 16, 42, 11, 1, 31, 11, 6, 10, 19, 15, 32, 42, 14, 17, 11, 22, 6, 28, 0, 0, 19, 3, 27, 24, 15, 6, 13, 32, 0, 15, 19, 13, 61, 18]
```

- 第四层计算

```python
# state
# 浮点
mem_state_float = np.concatenate([layer_out_3_float, before_state_float], axis=0).reshape(7, -1).transpose()
# 定点
mem_state = np.concatenate([layer_out_3_8, before_state], axis=0).reshape(7, -1).transpose()

# att
# 浮点计算
weight_4_float = np.load("save/Variable_0_float.npy")
layer_out_4_float = np.sum(np.multiply(weight_4_float, mem_state_float), axis=1)
# 量化: 已知第4层输出的数据格式为 Q5.2
layer_out_4_q = (layer_out_4_float * 2**2).astype(np.int32)
# weight_4: Q6  mem_state: Q3 ;  (Q6 * Q3)>>7
weight_4 = np.load("save/Variable_0.npy").astype(np.int32)
layer_out_4 = np.sum((np.multiply(weight_4, mem_state)>>7), axis=1)
```

```
layer_out_4_q = [0, 0, 0, 0, -6, 1, 0, -6, 0, 5, -4, 0, 0, 0, -6, -1, 18, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, 4, 2, 0, 4, 0, 0, -3, -1, 0, 0, 5, -3, -1, 7, 0, 4, 0, -5, 0, -5, 0, 0, 0, 1, 2, -1, 2, 1, -5, -11, 0, 3, 5, 0, 17, 2]

layer_out_4   = [0, -1, 0, -1, -6, 1, 0, -7, -1, 5, -5, 0, 0, 0, -6, -2, 17, 0, -2, -1, -1, 0, 0, 1, 0, 0, 0, -2, 3, 1, -1, 4, -1, -1, -3, -2, 0, 0, 5, -4, -2, 6, -1, 4, 0, -5, -1, -6, 0, 0, -1, 1, 2, -2, 2, 0, -6, -11, 0, 3, 5, -1, 17, 1]
```


- 第五层计算

```python
# 浮点计算
weight_5_float = np.load("save/Variable_1_0_float.npy")
bias_5_float = np.load("save/Variable_2_0_float.npy")
layer_out_5_float = np.matmul(weight_5_float, layer_out_4_float) + bias_5_float
# 量化: 已知第5层输出的数据格式为 Q5.2
layer_out_5_q = (layer_out_5_float * 2**2).astype(np.int32)
# weight_5: Q7  bias_5: Q7 ;  (Q7 * Q2 + Q7<<2)>>7
weight_5 = np.load("save/Variable_1_0.npy").astype(np.int32)
bias_5 = np.load("save/Variable_2_0.npy").astype(np.int32)
layer_out_5 = (np.matmul(weight_5, layer_out_4_8) + (bias_5<<2))>>7

# Q9
layer_out_5_q9 = np.matmul(weight_5, layer_out_4_8) + (bias_5<<2)

```

```
layer_out_5_q = [0, 5, -5, -14, -3, 2, -3, -7, -3, -3, 4, -4, -8, -3, -3, -6, 0, -3, 1, -7, -2, 4, -1, -8, -9, -12, -3, -9, -13, -5, 5, 0, -3,  0, -6, -1, 4, -14, -4, 3, -6, 3, -7, -5, -3, -4, -4, 6, -3, -5, -6, 2, 7, -3, -3, -10, 0, 7, -6, -7, 0,  0, -4,  -9, -6, 43]

layer_out_5   = [0, 5, -6, -15, -4, 4, -4, -7, -4, -5, 5, -4, -7, -2, -4, -7, 0, -3, 0, -7, -1, 3, -2, -8, -8, -12, -2, -8, -12, -6, 6, 0, -3, -1, -8, -2, 2, -14, -5, 3, -5, 1, -7, -5, -3, -5, -3, 5, -5, -6, -8, 0, 6, -6, -4, -11, 0, 8, -6, -9, 1, -1, -7, -10, -7, 39]
```

- softmax

```python
#浮点计算: 减去最大值,防止溢出
out_exp_float = np.exp(layer_out_5_float - layer_out_5_float.max())

out_sum_float = np.sum(out_exp_float)
prob_float = out_exp_float / out_sum_float
# Q8
prob_float_q = (prob_float * (2 ** 8)).astype(np.int32)


def iexp(x, Q):
    tmp = x / (2**Q)
    r = np.exp(tmp)
    int_r = int(r * (2**8))
    return int_r
# 定点计算: 减去最大值, 防止溢出
exp_int_func = np.frompyfunc(iexp, 2, 1)
# Q2
out_exp = exp_int_func(layer_out_5 - layer_out_5.max(), 2)
# Q9
# out_exp = exp_int_func(layer_out_5_q9  - layer_out_5_q9.max(), 9)


out_exp_sum = np.sum(out_exp)

np.clip(((out_exp / out_exp_sum).astype(np.int32) << 8), a_max=255, a_min=0)
```

```
prob_float_q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255]

prob         = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255]
```