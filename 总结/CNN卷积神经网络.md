# 卷积神经网络（Convolutional  Neural Networks）

## 目录

- 人工神经网络
- 卷积神经网络

## 一、人工神经网络

​		传统的人工神经网络输入时二维的特征数据，在获得这些二维数据前，一般要对原始的数据进行复杂的特征工程来提取有效的特征。人工神经网络的一般结构如下图所示：

![](cnn_pic/2018021914554174.jpg)



## 二、卷积神经网络

### 2.1 结构

​		卷积网络的结构如下图所示：

![](cnn_pic/20180219145758102.jpg)

​		卷积神经网络是在三维上操作的，这是因为卷积神经网络利用输入是图片的特点，把神经元设计成三个维度 width,height,depth( 注意这个depth不是神经网络的深度，而是用来描述神经元的) 

![](cnn_pic/CNN计算过程.png)

### 2.2 卷积神经网络的内部过程

#### 2.2.1卷积层

```
卷积计算过程如下图所示，
```

​	输入：     7 x 7 x 3的图像，其中3为深度代表RGB三个颜色通道。
​	卷积核： 两个卷积核 W0 和 W1 维度为3 x 3 x 3 其中最后一位的3为深度，与输入数据的三个颜色通道一致；
​	计算：     单次卷积计算时，三个通道的卷积核分别与各自对应的三个颜色通道的输入数据进行卷积，然后将三个颜色通道的卷积结果求和就得到的卷积后的output volume的一个值。

![](cnn_pic\20180329152546883.gif)

```
- feature map :
	在输入层，如果是灰度图片，那就只有一个feature map；如果是彩色图片，一般就是3个feature map（红绿蓝）;
	层与层之间会有若干个卷积核（kernel）（也称为过滤器），上一层每个feature map跟每个卷积核做卷积，都会产生下一层的一个feature map，有N个卷积核，下层就会产生N个feather map, feather map的数量与卷积核的数量一致;
	多少个Feature Map的意义在于从多个角度去学习不同的特征；
	
- 卷积核（kernel）：
	卷积核也叫过滤器（filter），每个卷积核具有长宽深三个维度；
	在某个卷积层中，可以有多个卷积核：下一层需要多少个feather map，本层就需要多少个卷积核，即feather map的数量与卷积核的数量一致
	卷积核的长、宽都是人为指定的，长X宽也被称为卷积核的尺寸，常用的尺寸为3X3，5X5等；
	卷积核的深度与当前图像的深度（feather map的张数）相同，所以指定卷积核时，只需指定其长和宽 两个参数；
	
- 通道（channels）:
	输入层：输入为RGB图时，通道数为3；输入为单色图时，通道数为1；此时的通道数取决于输入数据的feather  map 数量；
	卷积核中：操作的图像数据的feather map张数；
	卷积输出：取决于卷积核的数量，即下层将产生的feather map张数；
	实际上，通道数是有feather map张数决定的，关键看是哪个位置的通道数
	
- 学习过程：
	卷积核实际上就是如3x3，5x5这样子的权值（weights）矩阵，网络要学习的，或者说要确定下来的，就是这些权值（weights）的数值。
```

#### 2.2.2 池化层

```
   在通过卷积获得了特征 (features) 之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如softmax分类器，但这样做计算量过大；为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，这种聚合的操作就叫做池化 (pooling)。池化可以将一幅大的图像缩小，同时又保留其中的重要信息，一般包含两种方法：最大池化（max-pooling）和 平均池化（mean-pooling）。
```

​		池化可以看成是特殊的卷积过程，两种池化方法的具体操作如下图所示：

![](cnn_pic/20180219154252145.jpg)

​		将feature map按照核的维度（一般是2 x 2）进行不重叠划分，每一片区域分别取最大或者均值保留，其余舍弃。这样就将原图缩减至原来的 1/4。

#### 2.2.3 激活层

```
   常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。
   ReLU（Rectified Linear Units）：f(x)=max(0,x)
```

#### 2.2.4 全连接层

```
将最后一层的feature Map 全部展开拼接成一个向量，然后进行预测，如下图所示：
```

![](cnn_pic/20180306182530284.jpg)



### 2.3 问题解析

#### 2.3.1 为什么卷积核是奇数

```
1:为了方便进行padding;
	奇数卷积核更容易做padding。我们假设卷积核大小为k*k，为了让卷积后的图像大小与原图一样大，根据公式可得到padding=（k-1）/2，这里的k只有在取奇数的时候，padding才能是整数，否则padding不好进行图片填充；
	
2:更容易找到锚点;
	在CNN中，一般会以卷积核的某个基准点进行窗口滑动，通常这个基准点是卷积核的中心点，所以如果k'是偶数，就找不到中心点了。（奇数相对于偶数，有中心点，对边沿、对线条更加敏感，可以更有效的提取边沿信息。保证了 padding 时，图像的两边依然相对称；偶数也可以使用，但是效率比奇数低。在数以万计或亿计的计算过程中，每个卷积核差一点，累计的效率就会差很多）；
```

#### 2.3.2 卷积核是不是都是正方形

```
不是，具体任务具体分析，很多情况下（语音，NLP等）使用的是长方形的卷积核；
```



## 参考

1：[概念解释](https://blog.csdn.net/xys430381_1/article/details/82529397)

2：[图解CNN](https://blog.csdn.net/v_JULY_v/article/details/79434745)

3：[卷积神经网络（CNN）的理解和实现](https://blog.csdn.net/qq_33414271/article/details/79337141)