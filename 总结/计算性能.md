# 计算性能评估

- 概念
- 计算



## 概念

### > 名词解释：

- `FLOPs`,  Floating point of operations. 是浮点运算次数，可以用来衡量算法/模型复杂度 

- ` FLOPS`, floating point operations per second. 每秒的浮点运算数，最后一个S表示的是秒，不能省略。
- `MACC`, multiply-accumulate operations.先乘起来再加起来的运算次数。

- `主频`, 处理器的工作频率也叫主频，包含外频和倍频两个部分，两者的乘积就是主频。
- `外频`, 处理器的基准频率，单位是MHz； 外频是处理器与主板之间同步运行的速度，而且绝大部分电脑系统中外频也是内存与主板之间的同步运行的速度，在这种方式下，可以理解为处理器的外频直接与内存相连通，实现两者间的同步运行状态 。
-  `倍频`, 全称为倍频系数，处理器的主频与外频之间存在着一个比值关系，这个比值就是倍频系数，简称倍频。

### > FLOPS换算：

- 一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算，
- 一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算，
- 一个TFLOPS（teraFLOPS）等于每秒一万亿（=10^12）次的浮点运算，
- 一个PFLOPS（petaFLOPS）等于每秒一千万亿（=10^15）次的浮点运算，
- 一个EFLOPS（exaFLOPS）等于每秒一百京（=10^18）次的浮点运算，
- 一个ZFLOPS（zettaFLOPS）等于每秒十万京（=10^21）次的浮点运算。

### > FLOPS与处理器主频的关系：

​		大多数时候以FLOPS值来衡量处理器每秒钟运行及计算次数，现今大部分的处理器中，都有一个专门用来处理浮点运算的“浮点运算器”（FPU）。因此FLOPS所量测的，实际上就是FPU的执行速度。

​		以主频为1GHZ的处理器为例进行说明：

​		`1GHz 就是每秒10亿个时钟周期，如果每个时钟周期能执行一次运算的话，就是每秒十亿次运算，如果每次运算能完成两个浮点操作，就是2G FLOPS（每秒二十亿次浮点操作）。`



## 计算：

```
加、减、乘、除、指数运算、平方根、最大值…都是一个FLOP

y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n-1]*x[n-1]

上面的运算有n次浮点乘法，n-1次浮点加法，所以总共FLOPS为2*n-1
先乘后加的浮点操作有n次，所以MACC为n
```

### > 激活层：

以 ReLU和sigmoid为例， 它们不做点积所以一般使用FLOPs进行度量；

- ReLU：
  $$
   y = max(x, 0)
  $$
  

   激活函数仅应用于层的输出，例如在具有 ![[公式]](https://www.zhihu.com/equation?tex=J) 个输出神经元的完全连接层上，ReLU计算 ![[公式]](https://www.zhihu.com/equation?tex=J) 次，因此我们将其判定为 ![[公式]](https://www.zhihu.com/equation?tex=J) FLOPS 

- sigmoid: 
  $$
  y = 1/(1+exp(-x))
  $$
   在Sigmoid激活函数中有四个不同的运算，因此将其判定为每个函数输出4 FLOPS或总层输出 ![[公式]](https://www.zhihu.com/equation?tex=J%5Ctimes+4) FLOPS 

### > 全连接层：

**一般形式**：`y = W*X + b`; 

输入是`N*D`, 权值矩阵 `D*O`, 输出 `N*O`; 

**参数量**： `D*O + b`(+`b`为偏置项)

**计算量**：
   `N`为batch_size, 默认为1，不影响网络体积，实际训练评估计算量时需要乘上N；
   隐藏层将`D`通道的数据映射为`O`通道，每个通道的计算包括`D`次乘法和`D-1`次加法，所以一个通道的计算量为`(2xD - 1)`,输出有`O`个通道，即总计算量：

不考虑偏置   `FLOPs = (2xD -1) x O`;   `MACC = D x O`

考虑偏置 `FLOPs = 2xDxO`; 

### > 卷积层：

- **常规二维卷积**

`feature map`的形状`(N, Cin, Hin, Win)`; `N`为batch_size,其中`Cin`为输入数据通道数；

卷积核的形状`(kH, kW)`；卷积核数量为`kn`;

输出feature map的形状`(N, Cout, Hout, Wout)`; `N`为batch_size，`Cout=kn`表示输出数据通道数；

stride = 1的情况下有

**参数量**： `kH*kW*Cin*Cout + Cout`； `Cout=kn`(+`Cout`为偏置项)

**计算量**：

不考虑偏置 `FLOPs =[（kH*kW*Cin) + (kH*kW*Cin - 1）]*Hout*Wout*Cout`

考虑偏置 `FLOPs = 2*kH*kW*Cin*Hout*Wout*Cout`

- **深度可分离卷积**

**` depthwise convolution`**

```
深度卷积层，类似于常规卷积，但是一个卷积核负责一个通道，一个通道只被一个卷积核卷积，输入通道数与输出通道数相同。

`feature map`的形状`(N, Cin, Hin, Win)`; `N`为batch_size,其中`Cin`为输入数据通道数；

卷积核的形状`(kH, kW)`；卷积核数量为`kn`;一般`kH=kW`;

输出feature map的形状`(N, Cout, Hout, Wout)`; `N`为batch_size，`Cout=kn`表示输出数据通道数；

参数量：`kH*kW*Cin`

计算量：`FLOPs = 2*kH*kW*Cin*Hout*Wout`
```

**`pointwise convolution`**

```
逐点卷积层，类似常规卷积，但是卷积核为`1x1`。

`feature map`的形状`(N, Cin, Hin, Win)`; `N`为batch_size,其中`Cin`为输入数据通道数；

卷积核的形状`(1, 1)`；卷积核数量为`kn`;

输出feature map的形状`(N, Cout, Hout, Wout)`; `N`为batch_size，`Cout=kn`表示输出数据通道数；

参数量：`Cin*Cout`

计算量：`FLOPs = 2*Cin*Hout*Wout*Cout`
```

**可分离卷积**

```
总参数量：`kH*kW*Cin + Cin*Cout`
总计算量：`FLOPs = 2*kH*kW*Cin*Hout*Wout + 2*Cin*Hout*Wout*Cout`
```



